{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617409a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1: Write a program to scheme a few activation functions that are used in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c100f29",
   "metadata": {
    "id": "2c100f29"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a105e4b",
   "metadata": {
    "id": "1a105e4b"
   },
   "outputs": [],
   "source": [
    "class AF():\n",
    "    def linear(x):\n",
    "     return x\n",
    "\n",
    "    def sigmoid(x):\n",
    "     return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def tanh(x):\n",
    "     return np.tanh(x)\n",
    "    \n",
    "    def RELU(x):\n",
    "        x1=[]\n",
    "        for i in x:\n",
    "            if i<0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "        return x1\n",
    "\n",
    "    def softmax(x):\n",
    "     return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982e4732",
   "metadata": {
    "id": "982e4732",
    "outputId": "25a70ea9-fb63-48c0-e93b-7afc72f26820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      " 0. to Cancel\n",
      " 1. Linear\n",
      " 2. Sigmoid\n",
      " 3.Tanh\n",
      " 4.ReLU\n",
      " 5.Softmax\n",
      " \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1304\\2551894967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter\\n 0. to Cancel\\n 1. Linear\\n 2. Sigmoid\\n 3.Tanh\\n 4.ReLU\\n 5.Softmax\\n \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    n = int(input(\"Enter\\n 0. to Cancel\\n 1. Linear\\n 2. Sigmoid\\n 3.Tanh\\n 4.ReLU\\n 5.Softmax\\n \"))\n",
    "    if n==0:\n",
    "        break\n",
    "    if n==1:\n",
    "        x = np.linspace(-10, 10)\n",
    "        plt.plot(x, AF.linear(x))\n",
    "        plt.title('Activation Function :Linear')\n",
    "        plt.show()\n",
    "    elif n==2:\n",
    "        x = np.linspace(-10, 10)\n",
    "        plt.plot(x, AF.sigmoid(x))\n",
    "        plt.title('Activation Function :Sigmoid')\n",
    "        plt.show()\n",
    "    elif n==3:\n",
    "        x = np.linspace(-10, 10)\n",
    "        plt.plot(x, AF.tanh(x))\n",
    "        plt.title('Activation Function :Tanh')\n",
    "        plt.show()\n",
    "    elif n==4:\n",
    "        x = np.linspace(-10, 10)\n",
    "        plt.plot(x, AF.RELU(x))\n",
    "        plt.title('Activation Function :ReLU')\n",
    "        plt.show()\n",
    "    elif n==5:\n",
    "        x = np.linspace(-10, 10)\n",
    "        plt.plot(x, AF.softmax(x))\n",
    "        plt.title('Activation Function :Softmax')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed7a78",
   "metadata": {
    "id": "5fed7a78"
   },
   "outputs": [],
   "source": [
    "# This code defines a class AF that contains different activation functions used in neural networks: linear, sigmoid, tanh, ReLU, and softmax. The class also contains a loop that prompts the user to choose an activation function and plots the chosen function using the matplotlib library. The loop continues until the user enters 0 to cancel. The linspace() function from the numpy library is used to create an array of evenly spaced values over a specified interval, which is used as the input to each activation function. The plt.plot() function is used to plot the output of each activation function, and plt.title() is used to set the title of the plot. Finally, plt.show() is used to display the plot."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
